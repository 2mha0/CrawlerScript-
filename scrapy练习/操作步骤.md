# Scrapy 框架的基本使用
## 创建工程
* 首先**创建一个工程**:`scrapy startproject xxxPro`
* 然后进入到该工程目录:`cd xxxPro`
* 在`spiders`子目录中**创建一个爬虫文件**:`scrapy genspider spiderName www.xxx.com`
* **执行工程:**`scrapy crawl spiderName`,如果不想输出日志信息，则在后面加入参数`--nolog`;如果只想输出错误信息则在`settings.py`中加入一行`LOG_LEVEL = 'ERROR'`

  * 也可以写**启动脚本：**

  begin.py:

  ```python
  from scrapy import cmdline
  cmdline.execute('scrapy crawl freePage'.split())
  ```

  
-----
## 工程的常见配置
* 不遵从君子协议:在`settings.py`中将`ROBOTSTXT_OBEY = False`
* 将`first.py`(新建爬虫文件)中的`allowed_domains`进行注释:爬取的资源不止局限于该域名下
* `first.py`中更改**UA伪装**:设置`USER_AGENT`的值
-----
## 对诗词名句网站的书目录进行数据解析
* **主要是对xpath进行回顾**
    * xpath返回的是列表，但是列表元素一定是Selector类型的对象
    * extract可以将Selector对象中data参数存储的字符串提取出来
    * 列表调用来extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来

**完整代码如下:**
```python
import scrapy

class FirstSpider(scrapy.Spider):
    # 爬虫文件的名称:就是爬虫源文件的一个唯一标识
    name = "first"
    # 允许的域名
    # allowed_domains = ["www.xxx.com"]
    # 起始的 url 列表:该列表中存放的 url 会被 scrapy 自动进行请求的发送
    start_urls = ["https://www.shicimingju.com/book/sanguoyanyi.html"]

    # 用作数据解析: response 参数表示的就是请求成功后对应的响应对象
    def parse(self, response):
        ul_list = response.xpath('//*[@id="main_left"]/div/div[4]/ul/li')
        i = 0
        for li in ul_list:
            print('第' + str(i) + '回 : ')
            # xpath返回的是列表，但是列表元素一定是Selector类型的对象
            # extract可以将Selector对象中data参数存储的字符串提取出来
            name = li.xpath('./a/text()')[0].extract()
            # 列表调用来extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来
            href = li.xpath('./a/@href').extract()[0]
            print(name)
            print(href)

```
-----
## scrapy持久化存储
### 基于终端指令
* 要求：只可以将parse方法的返回值存储到本地文本文件中
* 注意：持久化存储对应的文本文件的类型只可以为:`'json', 'jsonlines', 'jsonl', 'jl', 'csv', 'xml', 'marshal', 'pickle'`中的一种
* 指令`scrapy crawl xxx -o filePath`
* 好处:简洁高效便捷
* 缺点:局限性比较强(数据只可以存储在指定后缀的文本文件中)

代码:
```python
def parse(self, response):
    ul_list = response.xpath('//*[@id="main_left"]/div/div[4]/ul/li')
    # 用于保存所有的数据
    all_data = []
    for li in ul_list:
        # xpath返回的是列表，但是列表元素一定是Selector类型的对象
        # extract可以将Selector对象中data参数存储的字符串提取出来
        name = li.xpath('./a/text()')[0].extract()
        # 列表调用来extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来
        href = li.xpath('./a/@href').extract()[0]
        # print(name)
        # print(href)
        dic = {
            'name': name,
            'href': href
        }
        all_data.append(dic)

    return all_data
```

### 基于管道
* 编码流程：
  * 1.数据解析
  * 2.在`item`类中定义相关的属性
  * 3.将解析的数据封装存储到`item`类型的对象
  * 4.将`item`类型的对象提交给管道进行持久化存储的操作
  * 5.在管道类`process_item()`方法中要将其接收到的`item`对象存储的数据进行持久化存储操作
  * 6.在配置文件中开启管道
* 好处:
  * 通用性强

代码:
**first.py**
```python
def parse(self, response):
    ul_list = response.xpath('//*[@id="main_left"]/div/div[4]/ul/li')
    # 用于保存所有的数据
    all_data = []
    for li in ul_list:
        # xpath返回的是列表，但是列表元素一定是Selector类型的对象
        # extract可以将Selector对象中data参数存储的字符串提取出来
        name = li.xpath('./a/text()')[0].extract()
        # 列表调用来extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来
        href = li.xpath('./a/@href').extract()[0]
        # print(name)
        # print(href)
        item = FirstbloodItem()
        item['name'] = name
        item['href'] = href

        # 将item提交给了管道
        yield item
```

**items.py**
```python
import scrapy

class FirstbloodItem(scrapy.Item):
    # define the fields for your item here like:
    name = scrapy.Field()
    href = scrapy.Field()
    pass
```

**pipelines.py**
```python
class FirstbloodPipeline:
    fp = None
    # 重写父类的一个方法:该方法只有开始爬虫的时候被调用一次
    def open_spider(self, spider):
        print('开始爬虫……')
        self.fp = open('./data_SanGuo.txt', 'w', encoding='utf-8')

    # 该方法可以接收爬虫文件提交过来的 item 对象
    # 该方法接收到一个 item 就会被调用一次
    def process_item(self, item, spider):
        name = item['name']
        href = item['href']
        self.fp.write(name + '\t' + href + '\n')
        return item

    # 重写父类方法
    def close_spider(self, spider):
        print('结束爬虫!!!')
        self.fp.close()

```

**settings.py**
```python
ITEM_PIPELINES = {
   #  300 表示优先级，数值越小优先级越高
   "firstBlood.pipelines.FirstbloodPipeline": 300,
}
```


***将爬取到的数据一份存储到本地一份存储到数据库***(使用MYSQL)
* 管道文件中一个管道类对应的是将数据存储到一种平台
* 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接收
* `process_item`中的`return item`表示将item传递给下一个即将被执行的管道类

**代码:**
**pipelines.py**
```python
class mysqlPileLine(object):
    conn = None
    cursor = None

    def open_spider(self, spider):
        self.conn = pymysql.Connect(host='127.0.0.1', port=3306, user='root', password='15023119606', db='test')

    def process_item(self, item, spider):
        self.cursor = self.conn.cursor()
        try:
            self.cursor.execute('insert into SanGuo values("%s","%s")'%(item['name'], item['href']))
            self.conn.commit()
        except Exception as e:
            print(e)
            self.conn.rollback()

        return item

    def close_spider(self, spider):
        self.cursor.close()
        self.conn.close()
```
**settings.py**
```python
ITEM_PIPELINES = {
   #  300 表示优先级，数值越小优先级越高
   "firstBlood.pipelines.FirstbloodPipeline": 300,
   "firstBlood.pipelines.mysqlPileLine": 301,
}
```
-----
## 全站数据爬取

* 使用回调机制来爬取有很多页面的信息

**代码:**

**getTitle.py**

```python
import scrapy
from ..items import FirstbloodItem

class GettitleSpider(scrapy.Spider):
    name = "getTitle"
    # allowed_domains = ["www.baidu.com"]
    start_urls = ["http://www.521609.com/vodtype/6.html"]

    # url 模板
    url_template = 'http://www.521609.com/vodtype/6-%d.html'
    page = 2

    def parse(self, response):
        list_li = response.xpath('/html/body/div[1]/div[2]/div[1]/div/div/div[2]/ul/li')
        for li in list_li:
            title = li.xpath('./div/div/h4/a/text()').extract()[0]
            # print(title)
            item = FirstbloodItem()
            item['title'] = title
            yield item
        if self.page <= 125:
            new_url = format(self.url_template%self.page)
            self.page += 1

            yield scrapy.Request(url=new_url, callback=self.parse)
```

**items.py**

```python
import scrapy
class FirstbloodItem(scrapy.Item):
    # define the fields for your item here like:
    title = scrapy.Field()
    pass
```

**pipelines.py**

```python
class FirstbloodPipeline:
    fp = None
    # 重写父类的一个方法:该方法只有开始爬虫的时候被调用一次
    def open_spider(self, spider):
        print('开始爬虫……')
        self.fp = open('./data_movieTitle.txt', 'w', encoding='utf-8')

    # 该方法可以接收爬虫文件提交过来的 item 对象
    # 该方法接收到一个 item 就会被调用一次
    def process_item(self, item, spider):
        title = item['title']
        self.fp.write(title + '\n')
        # 传递给下一个即将被执行的管道类
        return item

    # 重写父类方法
    def close_spider(self, spider):
        print('结束爬虫!!!')
        self.fp.close()
```

**settings.py**

```python
ITEM_PIPELINES = {
   #  300 表示优先级，数值越小优先级越高
   "firstBlood.pipelines.FirstbloodPipeline": 300,
}
```

-----

## 请求传参（深度爬取页面时）

* 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
* 需求：爬取免费简历模板的名字与url

**代码：**

**begin.py**：项目的启动文件

```python
from scrapy import cmdline
cmdline.execute('scrapy crawl freePage'.split())
```

**freePage.py**：主要的逻辑实现

```python
import scrapy
from ..items import ResumeproScrapyItem

class FreepageSpider(scrapy.Spider):
    name = "freePage"
    # allowed_domains = ["www.xxx.com"]
    start_urls = ["https://sc.chinaz.com/jianli/free.html"]
    url_template = 'https://sc.chinaz.com/jianli/free_%d.html'
    page_num = 2

    def parse_detail(self, response):
        download_all = response.xpath('//*[@id="down"]')
        for li in download_all:
            item = response.meta['item']
            download_href = li.xpath('.//li[1]/a/@href').extract_first()
            # print(download_href)
            item['download_href'] = download_href
            yield item

    def parse(self, response):
        resume_all = response.xpath('//*[@id="container"]/div')
        for li in resume_all:
            name = li.xpath('./a/img/@alt').extract_first()
            # print(name)
            detail_href = li.xpath('./a/@href').extract_first()
            item = ResumeproScrapyItem()
            item['name'] = name
            item['detail_href'] = detail_href
            # print(detail_href)
            yield scrapy.Request(url=detail_href, callback=self.parse_detail, meta={'item': item})

            # 分页操作
            if self.page_num <= 3:
                new_url = format(self.url_template%self.page_num)
                self.page_num += 1
                yield scrapy.Request(url=new_url, callback=self.parse)


```

**items.py**

```python
import scrapy


class ResumeproScrapyItem(scrapy.Item):
    # define the fields for your item here like:
    name = scrapy.Field()
    download_href = scrapy.Field()
    detail_href = scrapy.Field()
```

**pipelines.py**

```python
class ResumeproScrapyPipeline:
    fp = None
    def open_spider(self, spider):
        print('开始爬虫……')
        # self.fp = open('./res/%s.rar', 'wb')

    def process_item(self, item, spider):
        name = item['name']
        download_href = item['download_href']
        print(name + ':' + download_href)
        return item
```

-----

## 图片爬取（继承ImagesPipeline类）

### 基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别

* 字符串：只需要基于xpath进行解析且提交管道进行持久化存储
* 图片：xpath解析出图片src的属性值。单独的对图片地址发起请求获取图片二进制类型的数据

### 爬取站长素材中的图片

* **需求：**爬取`https://sc.chinaz.com/tupian/fengjing.html`中的图片

* **ImagesPipeline：**只需要将img的src属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制信息

* **使用流程：**

  * 数据解析（图片的地址）

  * 将存储图片地址的item提交到指定的管道类

  * 在管道文件中自定义一个继承`ImagesPipeline`的一个管道类，然后重写以下方法：

    | 函数                                                         | 作用                                                         |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | `get_media_requests(self, item, info)`                       | 根据图片地址对图片进行请求                                   |
    | `file_path(self, request, response=None, info=None, *, item=None)` | 指定图片的存储路径                                           |
    | `item_completed(self, results, item, info)`                  | 如果没有一下个管道类可不重写此方法，返回给下一个即将被执行的管道类，类似于process_item方法 |

  * 在配置文件中进行配置：

        1. 指定图片存储的目录：`IMAGES_STORE = './imgs_FengJing'`
        2. 指定开启的管道：自定义的管道类

### 代码

**img.py**

```python
import scrapy
from ..items import ImgsproItem

class ImgSpider(scrapy.Spider):
    name = "img"
    # allowed_domains = ["www.xxx.com"]
    start_urls = ["https://sc.chinaz.com/tupian/fengjing.html"]

    def parse(self, response):
        item = ImgsproItem()
        div_list = response.xpath('/html/body/div[3]/div[2]/div')
        for li in div_list:
            href = 'https:' + li.xpath('./img/@data-original').extract_first()
            name = li.xpath('./img/@alt').extract_first() + '.jpg'
            item['href'] = href
            item['name'] = name
            print(name + ':' + href)
            yield item

```

**items.py**

```python
import scrapy


class ImgsproItem(scrapy.Item):
    # define the fields for your item here like:
    name = scrapy.Field()
    href = scrapy.Field()
```

**pipelines.py**

```python
import scrapy
# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
from scrapy.pipelines.images import ImagesPipeline

# 继承了该类需要重写三个父类方法
class ImgsproPipeline(ImagesPipeline):
    # 根据图片地址对图片进行请求
    def get_media_requests(self, item, info):
        # 通过meta来将文件名进行传递
        yield scrapy.Request(item['href'], meta={'name': item['name']})

    # 指定图片的存储路径
    def file_path(self, request, response=None, info=None, *, item=None):
        # 由于item不可用，因此用request的meta来将名字进行传递
        return request.meta['name']
        # imgName = request.url.split('/')[-1]
        # return imgName

    # 如果没有一下个管道类可不重写此方法
    def item_completed(self, results, item, info):
        # 返回给下一个即将被执行的管道类，类似于process_item方法
        return item

```

**settings.py**（部分）

```python
ITEM_PIPELINES = {
   "imgsPro.pipelines.ImgsproPipeline": 300,
}
# 指定图片的存储目录
IMAGES_STORE = './imgs_FengJing'
```



