# Scrapy 框架的基本使用
## 创建工程
* 首先**创建一个工程**:`scrapy startproject xxxPro`
* 然后进入到该工程目录:`cd xxxPro`
* 在`spiders`子目录中**创建一个爬虫文件**:`scrapy genspider spiderName www.xxx.com`
* **执行工程:**`scrapy crawl spiderName`,如果不想输出日志信息，则在后面加入参数`--nolog`;如果只想输出错误信息则在`settings.py`中加入一行`LOG_LEVEL = 'ERROR'`
-----
## 工程的常见配置
* 不遵从君子协议:在`settings.py`中将`ROBOTSTXT_OBEY = False`
* 将`first.py`(新建爬虫文件)中的`allowed_domains`进行注释:爬取的资源不止局限于该域名下
* `first.py`中更改**UA伪装**:设置`USER_AGENT`的值
-----
## 对诗词名句网站的书目录进行数据解析
* **主要是对xpath进行回顾**
    * xpath返回的是列表，但是列表元素一定是Selector类型的对象
    * extract可以将Selector对象中data参数存储的字符串提取出来
    * 列表调用来extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来

**完整代码如下:**
```python
import scrapy

class FirstSpider(scrapy.Spider):
    # 爬虫文件的名称:就是爬虫源文件的一个唯一标识
    name = "first"
    # 允许的域名
    # allowed_domains = ["www.xxx.com"]
    # 起始的 url 列表:该列表中存放的 url 会被 scrapy 自动进行请求的发送
    start_urls = ["https://www.shicimingju.com/book/sanguoyanyi.html"]

    # 用作数据解析: response 参数表示的就是请求成功后对应的响应对象
    def parse(self, response):
        ul_list = response.xpath('//*[@id="main_left"]/div/div[4]/ul/li')
        i = 0
        for li in ul_list:
            print('第' + str(i) + '回 : ')
            # xpath返回的是列表，但是列表元素一定是Selector类型的对象
            # extract可以将Selector对象中data参数存储的字符串提取出来
            name = li.xpath('./a/text()')[0].extract()
            # 列表调用来extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来
            href = li.xpath('./a/@href').extract()[0]
            print(name)
            print(href)

```
-----
## scrapy持久化存储
### 基于终端指令
* 要求：只可以将parse方法的返回值存储到本地文本文件中
* 注意：持久化存储对应的文本文件的类型只可以为:`'json', 'jsonlines', 'jsonl', 'jl', 'csv', 'xml', 'marshal', 'pickle'`中的一种
* 指令`scrapy crawl xxx -o filePath`
* 好处:简洁高效便捷
* 缺点:局限性比较强(数据只可以存储在指定后缀的文本文件中)

代码:
```python
def parse(self, response):
    ul_list = response.xpath('//*[@id="main_left"]/div/div[4]/ul/li')
    # 用于保存所有的数据
    all_data = []
    for li in ul_list:
        # xpath返回的是列表，但是列表元素一定是Selector类型的对象
        # extract可以将Selector对象中data参数存储的字符串提取出来
        name = li.xpath('./a/text()')[0].extract()
        # 列表调用来extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来
        href = li.xpath('./a/@href').extract()[0]
        # print(name)
        # print(href)
        dic = {
            'name': name,
            'href': href
        }
        all_data.append(dic)

    return all_data
```

### 基于管道
* 编码流程：
  * 1.数据解析
  * 2.在`item`类中定义相关的属性
  * 3.将解析的数据封装存储到`item`类型的对象
  * 4.将`item`类型的对象提交给管道进行持久化存储的操作
  * 5.在管道类`process_item()`方法中要将其接收到的`item`对象存储的数据进行持久化存储操作
  * 6.在配置文件中开启管道
* 好处:
  * 通用性强

代码:
**first.py**
```python
def parse(self, response):
    ul_list = response.xpath('//*[@id="main_left"]/div/div[4]/ul/li')
    # 用于保存所有的数据
    all_data = []
    for li in ul_list:
        # xpath返回的是列表，但是列表元素一定是Selector类型的对象
        # extract可以将Selector对象中data参数存储的字符串提取出来
        name = li.xpath('./a/text()')[0].extract()
        # 列表调用来extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来
        href = li.xpath('./a/@href').extract()[0]
        # print(name)
        # print(href)
        item = FirstbloodItem()
        item['name'] = name
        item['href'] = href

        # 将item提交给了管道
        yield item
```

**items.py**
```python
import scrapy

class FirstbloodItem(scrapy.Item):
    # define the fields for your item here like:
    name = scrapy.Field()
    href = scrapy.Field()
    pass
```

**pipelines.py**
```python
class FirstbloodPipeline:
    fp = None
    # 重写父类的一个方法:该方法只有开始爬虫的时候被调用一次
    def open_spider(self, spider):
        print('开始爬虫……')
        self.fp = open('./data_SanGuo.txt', 'w', encoding='utf-8')

    # 该方法可以接收爬虫文件提交过来的 item 对象
    # 该方法接收到一个 item 就会被调用一次
    def process_item(self, item, spider):
        name = item['name']
        href = item['href']
        self.fp.write(name + '\t' + href + '\n')
        return item

    # 重写父类方法
    def close_spider(self, spider):
        print('结束爬虫!!!')
        self.fp.close()

```

**settings.py**
```python
ITEM_PIPELINES = {
   #  300 表示优先级，数值越小优先级越高
   "firstBlood.pipelines.FirstbloodPipeline": 300,
}
```


***将爬取到的数据一份存储到本地一份存储到数据库***(使用MYSQL)
* 管道文件中一个管道类对应的是将数据存储到一种平台
* 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接收
* `process_item`中的`return item`表示将item传递给下一个即将被执行的管道类

**代码:**
**pipelines.py**
```python
class mysqlPileLine(object):
    conn = None
    cursor = None

    def open_spider(self, spider):
        self.conn = pymysql.Connect(host='127.0.0.1', port=3306, user='root', password='15023119606', db='test')

    def process_item(self, item, spider):
        self.cursor = self.conn.cursor()
        try:
            self.cursor.execute('insert into SanGuo values("%s","%s")'%(item['name'], item['href']))
            self.conn.commit()
        except Exception as e:
            print(e)
            self.conn.rollback()

        return item

    def close_spider(self, spider):
        self.cursor.close()
        self.conn.close()
```
**settings.py**
```python
ITEM_PIPELINES = {
   #  300 表示优先级，数值越小优先级越高
   "firstBlood.pipelines.FirstbloodPipeline": 300,
   "firstBlood.pipelines.mysqlPileLine": 301,
}
```
-----
## 全站数据爬取

* 使用回调机制来爬取有很多页面的信息

**代码:**

**getTitle.py**

```python
import scrapy
from ..items import FirstbloodItem

class GettitleSpider(scrapy.Spider):
    name = "getTitle"
    # allowed_domains = ["www.baidu.com"]
    start_urls = ["http://www.521609.com/vodtype/6.html"]

    # url 模板
    url_template = 'http://www.521609.com/vodtype/6-%d.html'
    page = 2

    def parse(self, response):
        list_li = response.xpath('/html/body/div[1]/div[2]/div[1]/div/div/div[2]/ul/li')
        for li in list_li:
            title = li.xpath('./div/div/h4/a/text()').extract()[0]
            # print(title)
            item = FirstbloodItem()
            item['title'] = title
            yield item
        if self.page <= 125:
            new_url = format(self.url_template%self.page)
            self.page += 1

            yield scrapy.Request(url=new_url, callback=self.parse)
```

**items.py**

```python
import scrapy
class FirstbloodItem(scrapy.Item):
    # define the fields for your item here like:
    title = scrapy.Field()
    pass
```

**pipelines.py**

```python
class FirstbloodPipeline:
    fp = None
    # 重写父类的一个方法:该方法只有开始爬虫的时候被调用一次
    def open_spider(self, spider):
        print('开始爬虫……')
        self.fp = open('./data_movieTitle.txt', 'w', encoding='utf-8')

    # 该方法可以接收爬虫文件提交过来的 item 对象
    # 该方法接收到一个 item 就会被调用一次
    def process_item(self, item, spider):
        title = item['title']
        self.fp.write(title + '\n')
        # 传递给下一个即将被执行的管道类
        return item

    # 重写父类方法
    def close_spider(self, spider):
        print('结束爬虫!!!')
        self.fp.close()
```

**settings.py**

```python
ITEM_PIPELINES = {
   #  300 表示优先级，数值越小优先级越高
   "firstBlood.pipelines.FirstbloodPipeline": 300,
}
```

-----

## 请求传参（深度爬取页面时）

* 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
* 需求：爬取免费简历模板的名字与url

**代码：**

**begin.py**：项目的启动文件

```python
from scrapy import cmdline
cmdline.execute('scrapy crawl freePage'.split())
```

**freePage.py**：主要的逻辑实现

```python
import scrapy
from ..items import ResumeproScrapyItem

class FreepageSpider(scrapy.Spider):
    name = "freePage"
    # allowed_domains = ["www.xxx.com"]
    start_urls = ["https://sc.chinaz.com/jianli/free.html"]
    url_template = 'https://sc.chinaz.com/jianli/free_%d.html'
    page_num = 2

    def parse_detail(self, response):
        download_all = response.xpath('//*[@id="down"]')
        for li in download_all:
            item = response.meta['item']
            download_href = li.xpath('.//li[1]/a/@href').extract_first()
            # print(download_href)
            item['download_href'] = download_href
            yield item

    def parse(self, response):
        resume_all = response.xpath('//*[@id="container"]/div')
        for li in resume_all:
            name = li.xpath('./a/img/@alt').extract_first()
            # print(name)
            detail_href = li.xpath('./a/@href').extract_first()
            item = ResumeproScrapyItem()
            item['name'] = name
            item['detail_href'] = detail_href
            # print(detail_href)
            yield scrapy.Request(url=detail_href, callback=self.parse_detail, meta={'item': item})

            # 分页操作
            if self.page_num <= 3:
                new_url = format(self.url_template%self.page_num)
                self.page_num += 1
                yield scrapy.Request(url=new_url, callback=self.parse)


```

**items.py**

```python
import scrapy


class ResumeproScrapyItem(scrapy.Item):
    # define the fields for your item here like:
    name = scrapy.Field()
    download_href = scrapy.Field()
    detail_href = scrapy.Field()
```

**pipelines.py**

```python
class ResumeproScrapyPipeline:
    fp = None
    def open_spider(self, spider):
        print('开始爬虫……')
        # self.fp = open('./res/%s.rar', 'wb')

    def process_item(self, item, spider):
        name = item['name']
        download_href = item['download_href']
        print(name + ':' + download_href)
        return item
```



